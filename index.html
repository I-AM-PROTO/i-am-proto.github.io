<!DOCTYPE HTML>
<html lang="en">
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-EWKS5LLCSE"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-EWKS5LLCSE');
  </script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate"/>
  <meta http-equiv="Pragma" content="no-cache"/>
  <meta http-equiv="Expires" content="0"/>

  <title>Donghu Kim</title>

  <meta name="author" content="Donghu Kim">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%221.0em%22 font-size=%2280%22>üê≠</text></svg>">

</head>

<body>
  <div class="container">
    <div class="row" style="margin-top: 10px;">
      <div class="col-sm-4 name-column" style="min-width: 266px;">
        <p style="text-align:center">
          <name>Donghu Kim</name>
        </p>
      </div>
      <div class="col-sm-8 name-column text-right" style="min-width: 266px; margin-top: 10px">
        <p style="text-align:right">
            <a href="https://davian.kaist.ac.kr/"><img src="pictures/davian_logo.png" alt="logo_uni_tue" class="institute-logo-small"></a>
            &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
            <a href="https://gsai.kaist.ac.kr/?lang=eng/"><img src="pictures/kaistai_logo.png" alt="logo_uni_tue" class="institute-logo-medium"></a>
        </p>
      </div>
    </div>
    <div class="row common-rows">

      <div class="col-xs-12 col-sm-8 personal-column">
        <p>
          Hi there, welcome!
        </p>

        </p>
        <p> My name is Donghu Kim. I am on a Master's Degree program in KAIST (advised by
          <a href="https://sites.google.com/site/jaegulchoo/">Jaegul Choo</a>),
          studying reinforcement learning and embodied AI with these splendid researchers:
          <a href="https://lee15253.github.io/">Byungkun Lee</a>, 
          <a href="https://joonleesky.github.io/">Hojoon Lee</a>, 
          <a href="https://godnpeter.github.io/">Dongyoon Hwang</a>, 
          <a href="https://mynsng.github.io/">Hyunseung Kim</a>,
          <a href="https://kyungminn.github.io/">Kyungmin Lee</a>, and
          <a href="https://leeyngdo.github.io/">Youngdo Lee</a>.
        </p>
        
        <p>
        My research is focused in pushing the absolute limit of
        efficiency in RL for control: <strong>Can RL learn complex beahviors within 1K samples?
        Can we do it within an hour?</strong>
        As far fetched as the goal may seem, there are so many exciting components we can tackle,
        including feature learning, exploration, architecture design, optimizer, etc.
        </p>

        <p>
        I still have a long long way to go; if you want to discuss anything research related,
        I'd be more than happy to be engaged!
        </p>
        
        <p style="text-align:center">
          <a href="mailto:quagmire@kaist.ac.kr">Email</a> &nbsp/&nbsp
          <a href="CV.pdf">CV</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=LcYjQYcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
          <a href="https://github.com/i-am-proto/">Github</a>
        </p>

      </div>
      <div class="col-xs-12 col-sm-4 personal-column" style="margin-top: 30px">
        <img alt="profile photo" src="pictures/profile.jpg" class="personal-photo">
      </div>
    </div>
  </div>

  <br>

  <div class="container">
    <div class="row section-heading-rows">
      <h4>Work Experience</h4>
    </div>

    <!-------------------------------------------------------------->
  <div class="row common-rows" style="margin-top:-15px;">
    <div class="col-xs-12 col-sm-3 left-column">
        <div style="width: 100%; height: 150px; background-color: #ffffff; display: flex; justify-content: center; align-items: center; border-radius: 0px;">
              <img src="pictures/thumbs/krafton.png" alt="Krafton" style="max-width: 75%; max-height: 75%; object-fit: contain;">
        </div>
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <div style="display: flex; justify-content: space-between; align-items: baseline;">
          <strong style="font-size:18px;">Krafton AI</strong>
          <span style="white-space: nowrap;">June 2025 - Present</span>
        </div>
        <div style="display: flex; justify-content: space-between; align-items: baseline; font-size:16px;">
            Research Intern
            <!-- <span style="white-space: nowrap;">Seattle, USA</span> -->
        </div>
        <ul style="margin-left:-1.5em; margin-top:0.5em;">
        <li>Physical Intelligence Team (TBD)</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="container">
    <div class="row section-heading-rows">
      <h4>Publications</h4>
      <span class="publication-filter">
        [ <a href="javascript:void(0)" class="filter-btn active" data-filter="selected">Selected </a>/
        <a href="javascript:void(0)" class="filter-btn" data-filter="all">All</a> ]
      </span>
    </div>

    <!-------------------------------------------------------------->

    <div class="row common-rows publication-entry" style="margin-top: 3%">
      <div class="col-xs-12 custom-col-sm-3 left-column">
        <img src="pictures/thumbs/toxchat.png" alt="toxchat" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#b8fd5e">Langauge Agent</span>
        <br>
            <papertitle>
              Building Resource-Constrained Language Agents: A Korean Case Study on Chemical Toxicity Information
            </papertitle>
        <br>
        <a href="https://github.com/hojuncho">Hojun Cho</a>,
        <strong>Donghu Kim</strong>,
        <a href="https://dudrrm.github.io/">Soyoung Yang</a>,
        Chan Lee,
        Hunjoo Lee,
        <a href="https://sites.google.com/site/jaegulchoo">Jaegul Choo</a>.
        <br>
        <em>Under review</em>.
        <br>
        <a href="https://arxiv.org/abs/2503.17753v1">arXiv</a>
        <br><br>
        <p style="margin-top: -1%;"> 
            We build a light-weight LLM agent that answers chemical toxicity questions
            based on the Korean Tox-Info database.
        </p>
      </div>
    </div>

  <!-------------------------------------------------------------->
    
    <div class="row common-rows selected-publication" style="margin-top: 3%">
      <div class="col-xs-12 custom-col-sm-3 left-column">
        <img src="pictures/thumbs/simbav2.png" alt="simbav2" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#e2c7e5">Reinforcement Learning</span>
        <br>
            <papertitle>
                SimBaV2: Hyperspherical Normalization for Scalable Deep Reinforcement Learning
            </papertitle>
        <br>
        <a href="https://joonleesky.github.io/">Hojoon Lee</a>,
        <a href="https://https://leeyngdo.github.io/">Youngdo Lee</a>,
        <a href="https://takuseno.github.io/">Takuma Seno</a>,
        <strong>Donghu Kim</strong>,
        <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>,
        <a href="https://sites.google.com/site/jaegulchoo">Jaegul Choo</a>.
        <br>
        <em>NeurIPS'25, <b style="color:#fda333">Spotlight</b></em>.
        <br>
        <a href="https://arxiv.org/abs/2502.15280">arXiv</a> /
        <a href="https://dojeon-ai.github.io/SimbaV2/">project page</a> /
        <a href="https://github.com/dojeon-ai/SimbaV2">github</a>
        <br><br>
        <p style="margin-top: -1%;"> 
            We further regularize SimBa architecture by projecting both parameters and features onto a hypersphere,
            leading to better scaling properties in model size and compute.
        </p>
      </div>
    </div>

  <!-------------------------------------------------------------->
    
    <div class="row common-rows selected-publication" style="margin-top: 3%">
      <div class="col-xs-12 custom-col-sm-3 left-column">
        <img src="pictures/thumbs/simba.png" alt="simba" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#e2c7e5">Reinforcement Learning</span>
        <br>
            <papertitle>
                SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning
            </papertitle>
        <br>
        <a href="https://joonleesky.github.io/">Hojoon Lee</a>,
        <a href="https://godnpeter.github.io">Dongyoon Hwang</a>,
        <strong>Donghu Kim</strong>,
        <a href="https://mynsng.github.io">Hyunseung Kim</a>,
        <a href="https://taijunjet.com">Jun Jet Tai</a>,
        <a href="https://kausubbu.github.io">Kaushik Subramanian</a>,
        <a href="https://www.pwurman.org">Peter R. Wurman</a>,
        <a href="https://sites.google.com/site/jaegulchoo">Jaegul Choo</a>,
        <a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a>,
        <a href="https://takuseno.github.io/">Takuma Seno</a>.
        <br>
        <em>ICLR'25, <b style="color:#fda333">Spotlight</b></em>.
        <br>
        <a href="https://arxiv.org/abs/2410.09754">arXiv</a> /
        <a href="https://sonyresearch.github.io/simba/">project page</a> /
        <a href="https://github.com/SonyResearch/simba">github</a>
        <br><br>
        <p style="margin-top: -1%;"> 
            We propose a well-regularized architecture that avoids overfitting,
            allowing parameter and compute scale up in RL.
        </p>
      </div>
    </div>

  <!-------------------------------------------------------------->

    <div class="row common-rows publication-entry" style="margin-top: 3%">
        <div class="col-xs-12 custom-col-sm-3 left-column">
          <img src="pictures/thumbs/preprint2024dodont.png" alt="preprint2024dodont" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#e2c7e5">Reinforcement Learning</span>
        <span style="background-color:#ff9aa2">Skill Discovery</span>
        <br>
            <papertitle>
                Do‚Äôs and Don‚Äôts: Learning Desirable Skills with Instruction Videos
            </papertitle>
        <br>
        <a href="https://mynsng.github.io/">Hyunseung Kim</a>,
        <a href="https://lee15253.github.io/">Byungkun Lee</a>,
        <a href="https://joonleesky.github.io/">Hojoon Lee</a>,
        <a href="https://godnpeter.github.io/">Dongyoon Hwang</a>,
        <strong>Donghu Kim</strong>,
        <a href="https://sites.google.com/site/jaegulchoo/">Jaegul Choo</a>
        <br>
        <em>NeurIPS'24</em>.
        <br>
        <a href="https://arxiv.org/abs/2406.00324">arXiv</a> /
        <a href="http://mynsng.github.io/dodont">project page</a>
        <br><br>
        <p style="margin-top: -1%;"> 
            We present DoDont, a skill discovery algorithm that learns diverse behaviors 
            while following the behaviors in "do" videos while avoiding the behaviors in "don't" videos.
        </p>
      </div>
    </div>

    <div class="row common-rows selected-publication" style="margin-top: 3%">
      <div class="col-xs-12 custom-col-sm-3 left-column">
          <img src="pictures/thumbs/icml2024atari-pb.png" alt="icml2024atari-pb" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#e2c7e5">Reinforcement Learning</span>
        <span style="background-color:#ffdac1">Pre-training</span>
        <br>
          <papertitle>
            ATARI-PB: Investigating Pre-Training Objectives for Generalization in Pixel-Based RL
          </papertitle>
        <br>
        <strong>Donghu Kim*</strong>,
        <a href="https://joonleesky.github.io/">Hojoon Lee*</a>,
        <a href="https://kyungminn.github.io/">Kyungmin Lee*</a>,
        <a href="https://godnpeter.github.io/">Dongyoon Hwang</a>,
        <a href="https://sites.google.com/site/jaegulchoo/">Jaegul Choo</a>.        <br>
        <em>ICML'24</em>.
        <br>
        <a href="https://arxiv.org/abs/2406.06037">arXiv</a> /
        <a href="http://i-am-proto.github.io/atari-pb">project page</a> /
        <a href="https://github.com/dojeon-ai/Atari-PB">github</a> /
        <a href="https://icml.cc/media/PosterPDFs/ICML%202024/34150.png">poster</a>
        <br><br>
        <p style="margin-top: -1%;"> 
          We investigate which pre-training objectives are beneficial for in-distribution, near-out-of-distribution, and far-out-of-distribution generalization in visual reinforcement learning.
        </p>
      </div>
    </div>

    <div class="row common-rows publication-entry" style="margin-top: 3%">
        <div class="col-xs-12 custom-col-sm-3 left-column">
          <img src="pictures/thumbs/icml2024hnt.png" alt="icml2024hnt" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <span style="background-color:#e2c7e5">Reinforcement Learning</span>
        <span style="background-color:#b5ead7">Plasticity</span>
        <br>
        <papertitle>
            Slow and Steady Wins the Race: Maintaining Plasticity with Hare and Tortoise Networks
        </papertitle>
        <br>
        <a href="https://joonleesky.github.io/">Hojoon Lee</a>,
        Hyeonseo Cho,
        <a href="https://mynsng.github.io/">Hyunseung Kim</a>,
        <strong>Donghu Kim</strong>,
        Dugki Min,
        <a href="https://sites.google.com/site/jaegulchoo/">Jaegul Choo</a>,
        <a href="https://clarelyle.com/">Clare Lyle</a>.
        <br>
        <em>ICML'24</em>.
        <br>
        <a href="https://arxiv.org/abs/2406.02596">arXiv</a> /
        <a href="https://github.com/dojeon-ai/Hare-Tortoise">github</a>
        <br><br>
        <p style="margin-top: -1%;"> 
            To allow the network to continually adapt and generalize, we introduce Hare and Tortoise architecture, 
            inspired by the complementary learning system of the human brain.
        </p>
      </div>
    </div>
  </div>

</br>

  <div class="container">
    <div class="row section-heading-rows">
      <h4>Study Materials</h4>
    </div>
    <div>
      <p>
        <em>Note: These slides are made for studying purposes only, and likely have got something wrong here and there.
        If you happen to find some, feel free to make fun of me via e-mail :).</em>
      </p>
      <p>
        <ul>
          <li>25.03.28: MaestroMotif: LLM-assisted Skill Design <a href="slides/MaestroMotif.pdf">[slides]</a></li>
          <li>25.01.03: Warm Start RL <a href="slides/Warm Start RL.pdf">[slides]</a>
          <li>24.09.20: Catastrophic Interference in RL <a href="slides/Catastrophic Interference in RL.pdf">[slides]</a>
          <li>24.09.06: Understanding Self-Predictive RL <a href="slides/Understanding Self-Predictive RL.pdf">[slides]</a>
          <li>24.06.21: Automatic Environment Shaping <a href="slides/Automatic Environment Shaping.pdf">[slides]</a>
          <li>24.04.05: Stop Regressing (HL Gauss) <a href="slides/HLGauss.pdf">[slides]</a>
          <li>24.03.08: TD7 <a href="slides/TD7.pdf">[slides]</a>
          <li>24.02.23: Introduction to RL (CS285 Lecture2) <a href="slides/Introduction to RL.pdf">[slides]</a>
          <li>23.11.17: MAE Survey <a href="slides/MAE_Survey.pdf">[slides]</a>
          <li>23.09.01: ACRO (Multi-step IDM) <a href="slides/ACRO.pdf">[slides]</a>
        </ul>
      </p>

    </div>
  </div>

  <div class="container">
    <div class="row section-heading-rows">
      <h4>Honors & Awards</h4>
    </div>
    <div class="row" style="margin-top:-10px">
      <p>
        <ul>
          <li> Academic Excellence Award, Korea University 2019, 2022.
          <li> 2nd Place in NC AI Fellowship ($2000), NCSoft, 2019.
          <li> Presidential Science Scholarship (Total $40000), Korea Student Aid Foundation, 2018.
        </ul>
      </p>
    </div>
  </div>

  <div class="container">
    <div class="row section-heading-rows">
      <h4>Services (Reviewer)</h4>
    </div>
    <div class="row" style="margin-top:-10px">
      <p>
        <ul>
          <li> NeurIPS (2025)
        </ul>
      </p>
    </div>
  </div>

<!-- 
  <div class="container">
    <div class="row section-heading-rows">
      <h4>Other activities</h4>
    </div>
    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/review.png" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9 right-column">
        <papertitle>Reviewing activities
        </papertitle>
        <br>
        <ul>
          <li>Serving as a reviewer for NeurIPS'23, ICML'24, ICLR'24.</li>
        </ul>
      </div>
    </div>
    
    <div class="row common-rows">
      <div class="col-xs-12 col-sm-3 left-column">
        <img src="pictures/talk.png" class="paper-images">
      </div>
      <div class="col-xs-12 col-sm-9">
        <papertitle>Talks
        </papertitle>
        <br>
        <ul>
          <li>
            <a href="data/plastic2024talk.pdf">
              Towards Plastic Neural Network
            </a>.
            Sony AI, Tokyo, March 2024.
          </li>
          <li>
            <a href="data/plastic2024talk.pdf">
                Towards Plastic Neural Network
            </a>.
            Konkuk University DMIS Lab, Seoul, Feb 2024.
          </li>
          <li>
            <a href="data/pretrain2023talk.pdf">
                Pre-training for Intelligent Agent
            </a>.
            RL Korea, Seoul, Jan 2024.
          </li>
          <li>
            <a href="data/pretrain2023talk.pdf">
                Pre-training for Intelligent Agent
            </a>.
            Crevisse Partners, Seoul, Dec 2023.</li>
        </ul>
      </div>
    </div>
  </div> -->


  <div class="container">
    <div class="row">
      <div class="col">
        <p style="text-align:right;font-size:small;">
          Template based on <a href="https://joonleesky.github.io/">Hojoon Lee's website</a>.
        </p>
      </div>
    </div>
  </div>

<!-- Stolen from https://github.com/joonleesky/joonleesky.github.io/blob/master/index.html#L840 -->
<script>
  document.addEventListener('DOMContentLoaded', function() {
      const filterBtns = document.querySelectorAll('.filter-btn');
      const publications = document.querySelectorAll('.publication-entry');
      
      // Function to apply filtering
      function applyFilter(filterValue) {
        // Remove active class from all buttons
        filterBtns.forEach(btn => btn.classList.remove('active'));
        
        // Add active class to the button with matching data-filter
        const activeBtn = document.querySelector(`.filter-btn[data-filter="${filterValue}"]`);
        if (activeBtn) {
          activeBtn.classList.add('active');
        }
        
        // Show/hide publications based on filter
        if (filterValue === 'selected') {
          publications.forEach(pub => {
            if (pub.classList.contains('selected-publication')) {
              pub.classList.remove('hidden');
            } else {
              pub.classList.add('hidden');
            }
          });
        } else if (filterValue === 'all') {
          publications.forEach(pub => pub.classList.remove('hidden'));
        }
      }
      
      // Add click event listeners to filter buttons
      filterBtns.forEach(btn => {
        btn.addEventListener('click', function() {
          const filter = this.getAttribute('data-filter');
          applyFilter(filter);
        });
      });
      
      // Apply the "selected" filter by default on page load
      applyFilter('selected');
    });
</script>

</body>

</html>
